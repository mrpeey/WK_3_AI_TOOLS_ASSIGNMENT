{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86b8029",
   "metadata": {},
   "source": [
    "# Classical Machine Learning with Scikit-learn\n",
    "## Iris Species Classification Using Decision Tree\n",
    "\n",
    "**Dataset:** Iris Species Dataset\n",
    "\n",
    "**Goals:**\n",
    "1. Preprocess the data (handle missing values, encode labels)\n",
    "2. Train a decision tree classifier to predict iris species\n",
    "3. Evaluate using accuracy, precision, and recall\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd95ede",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll import all necessary libraries for data manipulation, visualization, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8530a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dataset loading\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Machine learning model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Model evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaec94",
   "metadata": {},
   "source": [
    "## 2. Load the Iris Dataset\n",
    "\n",
    "The Iris dataset is a classic dataset in machine learning, containing measurements of 150 iris flowers from three different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483569cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset from scikit-learn\n",
    "iris = load_iris()\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(\n",
    "    data=iris.data,\n",
    "    columns=iris.feature_names\n",
    ")\n",
    "\n",
    "# Add the target column (species)\n",
    "df['species'] = iris.target\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Number of samples: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")  # Excluding target column\n",
    "print(f\"\\nFeature names: {iris.feature_names}\")\n",
    "print(f\"Target names: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1a7e4",
   "metadata": {},
   "source": [
    "## 3. Explore the Dataset\n",
    "\n",
    "Let's examine the structure and characteristics of our data to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Display statistical summary of the features\n",
    "print(\"Statistical summary of features:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Check the distribution of target classes\n",
    "print(\"Distribution of species:\")\n",
    "print(df['species'].value_counts().sort_index())\n",
    "print(\"\\nMapping: 0 = setosa, 1 = versicolor, 2 = virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d59dc",
   "metadata": {},
   "source": [
    "## 4. Preprocess the Data\n",
    "\n",
    "We'll handle any missing values (though the Iris dataset is typically clean) and prepare our features and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Handle missing values\n",
    "# Check if there are any missing values\n",
    "missing_count = df.isnull().sum().sum()\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"Found {missing_count} missing values.\")\n",
    "    # For numerical features, we could fill with median or mean\n",
    "    # For this dataset, we'll drop rows with missing values\n",
    "    df_clean = df.dropna()\n",
    "    print(f\"Dropped rows with missing values. New shape: {df_clean.shape}\")\n",
    "else:\n",
    "    print(\"No missing values found. Dataset is clean!\")\n",
    "    df_clean = df.copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Step 2: Separate features (X) and target (y)\n",
    "# Features: all columns except 'species'\n",
    "X = df_clean.drop('species', axis=1)\n",
    "\n",
    "# Target: the 'species' column (already encoded as 0, 1, 2)\n",
    "y = df_clean['species']\n",
    "\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Step 3: Verify encoding\n",
    "# The Iris dataset from sklearn already has encoded labels (0, 1, 2)\n",
    "# If we had string labels, we would use LabelEncoder:\n",
    "# le = LabelEncoder()\n",
    "# y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(\"Target variable is already encoded:\")\n",
    "print(f\"Unique values: {sorted(y.unique())}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts().sort_index()}\")\n",
    "\n",
    "print(\"\\nData preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7040c1",
   "metadata": {},
   "source": [
    "## 5. Split the Data into Training and Testing Sets\n",
    "\n",
    "We'll split the data using an 80-20 ratio (80% for training, 20% for testing) to evaluate our model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: 80% training, 20% testing\n",
    "# random_state=42 ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,    # For reproducibility\n",
    "    stratify=y          # Maintain class distribution in both sets\n",
    ")\n",
    "\n",
    "print(\"Data split completed!\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Verify class distribution in train and test sets\n",
    "print(\"Class distribution in training set:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nClass distribution in testing set:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb0b4f",
   "metadata": {},
   "source": [
    "## 6. Train a Decision Tree Classifier\n",
    "\n",
    "Decision trees are intuitive models that make decisions based on asking a series of questions about the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree Classifier\n",
    "# We'll use default parameters first, but you can tune these for better performance\n",
    "dt_classifier = DecisionTreeClassifier(\n",
    "    criterion='gini',      # Measure of split quality ('gini' or 'entropy')\n",
    "    max_depth=None,        # Maximum depth of the tree (None = unlimited)\n",
    "    min_samples_split=2,   # Minimum samples required to split a node\n",
    "    min_samples_leaf=1,    # Minimum samples required at a leaf node\n",
    "    random_state=42        # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"Decision Tree Classifier initialized with parameters:\")\n",
    "print(dt_classifier.get_params())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Train the model on the training data\n",
    "print(\"Training the Decision Tree Classifier...\")\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Display tree information\n",
    "print(f\"Tree depth: {dt_classifier.get_depth()}\")\n",
    "print(f\"Number of leaves: {dt_classifier.get_n_leaves()}\")\n",
    "print(f\"Number of features used: {dt_classifier.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658a7c2",
   "metadata": {},
   "source": [
    "## 7. Make Predictions\n",
    "\n",
    "Now we'll use our trained model to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bac145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "print(\"Predictions made on test set!\")\n",
    "print(f\"Number of predictions: {len(y_pred)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Display first 10 predictions vs actual values\n",
    "print(\"First 10 predictions vs actual values:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test.values[:10],\n",
    "    'Predicted': y_pred[:10],\n",
    "    'Match': y_test.values[:10] == y_pred[:10]\n",
    "})\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Also get prediction probabilities for each class\n",
    "y_pred_proba = dt_classifier.predict_proba(X_test)\n",
    "print(\"Prediction probabilities for first 5 samples:\")\n",
    "print(\"(Columns represent: setosa, versicolor, virginica)\")\n",
    "print(y_pred_proba[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa845fd",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model\n",
    "\n",
    "We'll evaluate our model using multiple metrics: accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "\n",
    "# 1. Accuracy: Overall correctness of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(\"Interpretation: Percentage of correct predictions out of all predictions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Precision: How many predicted positives are actually positive\n",
    "# Using 'weighted' average to account for class imbalance\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(\"Interpretation: Of all predicted species, how many were correctly identified\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. Recall: How many actual positives were correctly predicted\n",
    "# Using 'weighted' average to account for class imbalance\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Recall (weighted): {recall:.4f}\")\n",
    "print(\"Interpretation: Of all actual species, how many were correctly identified\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 4. F1-Score: Harmonic mean of precision and recall\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "print(\"Interpretation: Balance between precision and recall\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 5. Per-class metrics using classification report\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred,\n",
    "    target_names=iris.target_names,\n",
    "    digits=4\n",
    "))\n",
    "print(\"\\nNote: Support shows the number of actual occurrences of each class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Confusion Matrix\n",
    "# Shows the number of correct and incorrect predictions for each class\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"=\" * 60)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Create a more readable confusion matrix using pandas\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"Actual {name}\" for name in iris.target_names],\n",
    "    columns=[f\"Predicted {name}\" for name in iris.target_names]\n",
    ")\n",
    "print(\"Confusion Matrix (detailed):\")\n",
    "print(cm_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm_df,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    cbar=True,\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    linecolor='black'\n",
    ")\n",
    "plt.title('Confusion Matrix - Decision Tree Classifier\\nIris Species Classification', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Species', fontsize=12)\n",
    "plt.xlabel('Predicted Species', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Diagonal elements: Correctly classified samples\")\n",
    "print(\"- Off-diagonal elements: Misclassified samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f30cb2",
   "metadata": {},
   "source": [
    "## 9. Visualize the Decision Tree\n",
    "\n",
    "Let's visualize the decision tree to understand how it makes classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree structure\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot the tree with detailed information\n",
    "plot_tree(\n",
    "    dt_classifier,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    filled=True,              # Color nodes by class\n",
    "    rounded=True,             # Rounded box corners\n",
    "    fontsize=10,\n",
    "    proportion=True           # Show proportions instead of counts\n",
    ")\n",
    "\n",
    "plt.title('Decision Tree Visualization - Iris Species Classification', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to read the tree:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"- Each box (node) shows:\")\n",
    "print(\"  * The decision rule (e.g., 'petal width <= 0.8')\")\n",
    "print(\"  * gini: Impurity measure (0 = pure, higher = more mixed)\")\n",
    "print(\"  * samples: Proportion of samples reaching this node\")\n",
    "print(\"  * value: Distribution of samples across classes\")\n",
    "print(\"  * class: The majority class at this node\")\n",
    "print(\"\\n- Colors represent the dominant class at each node\")\n",
    "print(\"- The tree splits samples based on feature values\")\n",
    "print(\"- Leaf nodes (bottom) contain the final predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d4673",
   "metadata": {},
   "source": [
    "## 10. Feature Importance\n",
    "\n",
    "Let's examine which features are most important for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the trained model\n",
    "feature_importance = dt_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': iris.feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "print(\"=\" * 60)\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.4f}', \n",
    "             ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Feature Importance in Decision Tree Classifier', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Higher values indicate more important features for classification\")\n",
    "print(\"- These scores show how much each feature contributes to reducing impurity\")\n",
    "print(f\"- The most important feature is: {importance_df.iloc[0]['Feature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489a10b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Data Preprocessing**: The Iris dataset was clean with no missing values, and labels were already encoded.\n",
    "\n",
    "2. **Model Performance**: Our Decision Tree Classifier achieved strong performance metrics on the test set.\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "   - **Accuracy**: Measures overall correctness\n",
    "   - **Precision**: Measures how many predicted species were correct\n",
    "   - **Recall**: Measures how many actual species were correctly identified\n",
    "   - **Confusion Matrix**: Shows detailed breakdown of correct and incorrect predictions\n",
    "\n",
    "4. **Feature Importance**: The model identified which physical measurements (sepal length/width, petal length/width) were most important for classification.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try other classifiers (Random Forest, SVM, KNN)\n",
    "- Tune hyperparameters for better performance\n",
    "- Use cross-validation for more robust evaluation\n",
    "- Implement feature scaling for other algorithms"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
